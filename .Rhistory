# Vote(Stacking) Classification
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from xgboost import XGBClassifier
from xgboost import plot_tree
sns.set()
## load breast cancer datset
df = np.loadtxt('data/diabetes.csv',delimiter=",")
df =  pd.read_csv('data/edhs.csv')
#Features Vectors for the first 50 observations
X =  df.drop(['Child_is_Alive_Out_Outcome_Variable'], axis=1).values[:100]
# Target Variable
y= df['Child_is_Alive_Out_Outcome_Variable'].values[:100]
# fit model on training data
model = XGBClassifier(n_estimators=100, use_label_encoder=False,eval_metric='logloss',learning_rate=0.2)
model.fit(X, y)
# plot single tree
plot_tree(model)
plt.show()
# fit model no training data
model =XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.2,eval_metric='logloss',use_label_encoder=False,verbose=False)
# Evaluation set
eval_set=[(X_train, y_train), (X_test, y_test)]
# Early stopping is added
model.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds=10)
# make predictions for test data
y_pred = model.predict(X_test)
y_pred = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
# Slit the dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=0.33,random_state=42)
# fit model no training data
model =XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.2,eval_metric='logloss',use_label_encoder=False,verbose=False)
# Evaluation set
eval_set=[(X_train, y_train), (X_test, y_test)]
# Early stopping is added
model.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds=10)
# make predictions for test data
y_pred = model.predict(X_test)
y_pred = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
# Slit the dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33,random_state=42)
# fit model no training data
model =XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.2,eval_metric='logloss',use_label_encoder=False,verbose=False)
# Evaluation set
eval_set=[(X_train, y_train), (X_test, y_test)]
# Early stopping is added
model.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds=10)
# make predictions for test data
y_pred = model.predict(X_test)
y_pred = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
# boosting Classification
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
sns.set()
df =  pd.read_csv('data/edhs.csv')
#Features Vectors for the first 50 observations
X =  df.drop(['Child_is_Alive_Out_Outcome_Variable'], axis=1).values[:100]
# Target Variable
y= df['Child_is_Alive_Out_Outcome_Variable'].values[:100]
# Slit the dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=0.33,random_state=42)
# fit model on training data
model = XGBClassifier(n_estimators=50, use_label_encoder=False,eval_metric='logloss',learning_rate=0.2)
model.fit(X_train, y_train)
# make predictions for test data
predictions = model.predict(X_test)
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# Slit the dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33,random_state=42)
# fit model no training data
model =XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.2,eval_metric='logloss',use_label_encoder=False,verbose=False)
# Evaluation set
eval_set=[(X_train, y_train), (X_test, y_test)]
# Early stopping is added
model.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds=10)
# make predictions for test data
y_pred = model.predict(X_test)
y_pred = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
# Slit the dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33,random_state=42)
# fit model no training data
model =XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.2,eval_metric='logloss',use_label_encoder=False,verbose=False)
# Evaluation set
eval_set=[(X_train, y_train), (X_test, y_test)]
# Early stopping is added
model.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds=10)
# make predictions for test data
y_pred = model.predict(X_test)
y_pred = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
# Slit the dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33,random_state=42)
# fit model no training data
model =XGBClassifier(n_estimators=100, max_depth=5,learning_rate=0.2,eval_metric='logloss',use_label_encoder=False,verbose=False)
# Evaluation set
eval_set=[(X_train, y_train), (X_test, y_test)]
# Early stopping is added
model.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds=10)
# make predictions for test data
y_pred = model.predict(X_test)
y_pred = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
results = model.evals_result()
plt.figure(figsize=(10,7))
plt.plot(results["validation_0"]["logloss"], label="Training loss")
plt.plot(results["validation_1"]["logloss"], label="Validation loss")
plt.axvline(31, color="gray", label="Optimal tree number");
plt.xlabel("Number of trees");
plt.ylabel("Log Loss");
plt.legend();
plt.savefig("fig/XGBClassifier_train_test_loss.png)
plt.show()
results = model.evals_result()
plt.figure(figsize=(10,7))
plt.plot(results["validation_0"]["logloss"], label="Training loss")
plt.plot(results["validation_1"]["logloss"], label="Validation loss")
plt.axvline(31, color="gray", label="Optimal tree number");
plt.xlabel("Number of trees");
plt.ylabel("Log Loss");
plt.legend();
plt.savefig("fig/XGBClassifier_loss.png)
plt.show()
reticulate::repl_python()
miniconda_update(path = miniconda_path())
library(reticulate)
miniconda_update(path = miniconda_path())
q()
install.packages("bookdown")
exit
q()
install.packages(c("kableExtra", "gridExtra","knitr", "ggthems","tidyverse"))
install.packages(c(""ggthemes","streamR","leaflet"))
install.packages(c("ggthemes","streamR","leaflet"))
q()
install.packages(c("ggExtra","jpeg"))
q()
serve_book(dir = ".", output_dir = "_book", preview = TRUE,
  in_session = TRUE, quiet = FALSE, ...)
install.packages('servr')
clear
serve_book(dir = ".", output_dir = "_book", preview = TRUE,
  in_session = TRUE, quiet = FALSE, ...)
library(servr)
serve_book(dir = ".", output_dir = "_book", preview = TRUE,
  in_session = TRUE, quiet = FALSE, ...)
exit
q()
library(servr)
q()
library(sf)
library(tidyverse)
data(HamiltonDAs)
install.packages("isdas")
library(broom)
install.packages(" nasaweather")
install.packages(" nasaweather")
install.packages("nasaweather")
q()
